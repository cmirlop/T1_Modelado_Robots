% ----------------------------------------------------------------------------------
% ----------------------------------------------------------------------------------
% ----------------------------------------------------------------------------------
% ----------------------------------------------------------------------------------
\chapter{Materiales y métodos}\label{cap:Sol}

\begin{Resumen}

    En este capítulo se describe la metodología integral adoptada para el desarrollo del sistema de navegación autónoma. En primer lugar, se presenta el **modelado dinámico** del robot diferencial utilizando el formalismo de Euler-Lagrange para obtener las ecuaciones de movimiento. Seguidamente, se detalla la estrategia de control basada en **Aprendizaje por Refuerzo Profundo (Deep Reinforcement Learning)**, definiendo la arquitectura del agente, el diseño de la función de recompensa y el procesamiento de la información sensorial (LIDAR y visión). A continuación, se describen las herramientas de **simulación e integración** empleadas (Unity, ROS, Keras/TensorFlow) para crear un entorno de entrenamiento realista. Finalmente, se expone el **procedimiento experimental**, incluyendo el diseño de escenarios y los criterios de validación de las políticas aprendidas.
    
\end{Resumen}

\section{Modelado Dinámico}

El objetivo de esta sección es obtener las ecuaciones que gobiernan el movimiento del robot JetBot 3 relacionando los pares aplicados por los motores con las aceleraciones del sistema. Para ello, se utiliza el formalismo de \textbf{Euler-Lagrange}.

\subsection{Hipótesis Adoptadas}
Para la derivación del modelo matemático, se asumen las siguientes simplificaciones sobre la física del robot, basadas en el modelo estándar de robot diferencial descrito en \cite{siegwart2011intro}:
\begin{itemize}
    \item \textbf{Cuerpo Rígido:} El chasis y las ruedas son indeformables.
    \item \textbf{Movimiento Plano:} El robot se desplaza sobre una superficie horizontal, por lo que la energía potencial gravitatoria es constante ($V=0$).
    \item \textbf{Rodadura Pura:} Se asume la condición de no deslizamiento longitudinal ni lateral en las ruedas (restricción no holonómica).
    \item \textbf{Simetría:} Se asume que el centro de masa (CoM) del robot se encuentra en el punto medio del eje que une las dos ruedas motrices.
\end{itemize}

\subsection{Definición de Coordenadas}
La postura del robot en el entorno global se define mediante el vector de coordenadas generalizadas $q$:

\begin{equation}
    q = \begin{bmatrix} x \\ y \\ \theta \end{bmatrix}
\end{equation}

Donde $(x, y)$ representan la posición cartesiana del punto medio del eje de las ruedas y $\theta$ la orientación del chasis respecto al eje $X$ global.

\begin{figure}[h]
    \centering
    % Aquí insertas tu diagrama generado con TikZ o la imagen
    \caption{Esquema cinemático del robot diferencial (JetBot) con las variables de estado y parámetros geométricos.}
    \label{fig:robot_scheme}
\end{figure}

\subsection{Formulación de Euler-Lagrange}
La función Lagrangiana $\mathcal{L}$ se define como la diferencia entre la energía cinética ($T$) y la energía potencial ($V$) del sistema:

\begin{equation}
    \mathcal{L}(q, \dot{q}) = T(q, \dot{q}) - V(q)
\end{equation}

Dado que el movimiento es plano ($V=0$), el Lagrangiano es igual a la energía cinética total, compuesta por la traslación lineal y la rotación angular:

\begin{equation}
    \mathcal{L} = \frac{1}{2} m (\dot{x}^2 + \dot{y}^2) + \frac{1}{2} I_z \dot{\theta}^2
\end{equation}

Siendo $m$ la masa total del robot e $I_z$ el momento de inercia respecto al eje vertical que pasa por el centro de masas.

Las ecuaciones de movimiento se obtienen aplicando la ecuación de Euler-Lagrange:

\begin{equation}
    \frac{d}{dt} \left( \frac{\partial \mathcal{L}}{\partial \dot{q}} \right) - \frac{\partial \mathcal{L}}{\partial q} = B(q)\tau - A^T(q)\lambda
\end{equation}

Donde $\tau$ son las fuerzas generalizadas (pares de los motores), $B(q)$ es la matriz de transformación de entrada y $A^T(q)\lambda$ representa las fuerzas de restricción (fricción de rodadura lateral) que limitan el movimiento a la dirección de las ruedas.

\subsection{Modelo Dinámico Resultante}
Resolviendo las derivadas y considerando la relación cinemática entre la velocidad del robot y la velocidad angular de las ruedas ($\dot{\phi}_R, \dot{\phi}_L$), se llega a la forma compacta del modelo dinámico en el espacio de estados:

\begin{equation}
    \begin{bmatrix} 
    m & 0 & 0 \\ 
    0 & m & 0 \\ 
    0 & 0 & I_z 
    \end{bmatrix}
    \begin{bmatrix} \ddot{x} \\ \ddot{y} \\ \ddot{\theta} \end{bmatrix}
    + \mathbf{F}_{fric}(\dot{q}) 
    = \frac{1}{R}
    \begin{bmatrix} 
    \cos\theta & \cos\theta \\ 
    \sin\theta & \sin\theta \\ 
    L/2 & -L/2 
    \end{bmatrix}
    \begin{bmatrix} \tau_R \\ \tau_L \end{bmatrix}
    \label{eq:final_dynamic}
\end{equation}

Donde:
\begin{itemize}
    \item $R$: Radio de las ruedas.
    \item $L$: Distancia entre las ruedas (ancho de vía).
    \item $\tau_R, \tau_L$: Pares aplicados por los motores derecho e izquierdo.
\end{itemize}

Esta ecuación (\ref{eq:final_dynamic}) describe cómo los pares de los motores aceleran al robot, sujeto a su masa e inercia \cite{siegwart2011intro}.
%Esta ecuación (\ref{eq:final_dynamic}) describe cómo los pares de los motores aceleran al robot, sujeto a su masa e inercia \cite{siegwart2011intro}.

\section{Metodología de control y aprendizaje por refuerzo (RL)}
Explicación de los esquemas de control diseñados: control clásico de referencia, control basado en RL, comparación, etc.
\subsection{Esquema de control}
\subsubsection{Control por LIDAR y cámara}
Este control se basa en el guiado por el entorno basandose en al información recibida por el LIDAR, 
basandose en los datos frontales y laterales que recibimos. Con la cámara lo que hacemos es buscar el color que
tiene el objetivo, en nuestro caso es el color rojo, por lo que se va analizando la imagen constantemente, ya que si detectamos
un área mínima del color del objetivo, extraemos cuanto estamos descentrados para poder centrarse y poder ir recto al 
objetivo.

\subsection{Algoritmo RL PPO}

\subsection{Algoritmo RL: PPO (Proximal Policy Optimization)}

\subsection*{Definición del Entorno y Agente}

El sistema implementado utiliza un enfoque de aprendizaje por refuerzo profundo (\textit{Deep Reinforcement Learning}). El agente, basado en el algoritmo PPO (Proximal Policy Optimization) \cite{schulman2017proximal}, interactúa con el entorno a través de un Modelo de Decisión de Markov (MDP). A diferencia de enfoques como DQN, PPO opera bajo una arquitectura Actor-Crítico, optimizando la política de control de manera iterativa y estable.

A continuación se detallan los tres componentes principales del MDP diseñado:

\subsubsection{Espacio de Acciones}
El control del robot se ha discretizado para facilitar el aprendizaje de la red neuronal. El espacio de acciones $\mathcal{A}$ consta de 7 comandos posibles, definidos por pares de velocidad lineal ($v$) y angular ($w$):

\begin{itemize}
    \item \textbf{Avance:}
    \begin{itemize}
        \item \texttt{forward}: Avance rápido ($v=3.0$ m/s, $w=0.0$ rad/s).
        \item \texttt{slow}: Avance lento ($v=1.0$ m/s, $w=0.0$ rad/s).
    \end{itemize}
    \item \textbf{Giros Estáticos:}
    \begin{itemize}
        \item \texttt{left}: Giro a la izquierda ($v=0.0$, $w=-1.0$).
        \item \texttt{right}: Giro a la derecha ($v=0.0$, $w=1.0$).
    \end{itemize}
    \item \textbf{Giros Suaves (Curvas):}
    \begin{itemize}
        \item \texttt{left\_soft}: Giro suave izquierda ($v=0.0$, $w=-0.5$).
        \item \texttt{right\_soft}: Giro suave derecha ($v=0.0$, $w=0.5$).
    \end{itemize}
    \item \textbf{Retroceso:}
    \begin{itemize}
        \item \texttt{back}: Maniobra de recuperación ($v=-0.1$, $w=0.0$).
    \end{itemize}
\end{itemize}

\subsubsection{Espacio de Observación (Estados)}
El vector de estado $s_t$ proporciona al agente una visión multimodal del entorno. Las observaciones se normalizan y concatenan para formar un vector de entrada de dimensión aproximada 188, compuesto por:

\begin{enumerate}
    \item \textbf{LiDAR ($L$):} Lecturas de distancia de 180 sectores. Los valores se normalizan al rango $[0, 1]$ dividiéndolos por la distancia máxima de recorte (\texttt{CLIP\_DIST} $= 30$ m).
    \item \textbf{Visión ($V$):} Información extraída del procesamiento de imagen para el objetivo rojo:
    \begin{itemize}
        \item \textit{Bearing} (Orientación): Valor continuo en $[-1, 1]$ indicando la desviación horizontal del objetivo respecto al centro.
        \item \textit{Área}: Tamaño relativo del objeto detectado, normalizado en $[0, 1]$.
    \end{itemize}
    \item \textbf{Flags de Bloqueo ($F$):} Indicadores binarios que alertan de obstáculos cercanos (distancia $< 5$ m) en las zonas frontal y laterales.
    \item \textbf{Odometría ($P$):} Coordenadas globales $(x, y, z)$ del robot.
\end{enumerate}

\subsubsection{Función de Recompensa}
La función de recompensa $R(s, a, s')$ es densa e incluye lógica basada en umbrales de seguridad definidos por el sensor LiDAR. 

\paragraph{Zonas de Seguridad (LiDAR)}
Se han definido tres zonas críticas basadas en la distancia frontal detectada:
\begin{itemize}
    \item \texttt{FRONT\_CLEAR} ($> 10$ m): Espacio frontal libre para navegar a alta velocidad.
    \item \texttt{FRONT\_DANGER} ($< 7$ m): Zona de precaución donde el avance es peligroso.
    \item \texttt{CRITICAL} ($< 5.5$ m): Distancia inminente de choque; se prioriza el retroceso.
\end{itemize}

\paragraph{Cálculo de Recompensas}
La recompensa total se asigna según la siguiente lógica priorizada:

\begin{enumerate}
    \item \textbf{Eventos Terminales:}
    \begin{itemize}
        \item \textbf{Éxito:} Si se alcanza el objetivo (área visual mínima detectada): $+30.0$.
        \item \textbf{Colisión:} Si el robot choca con un obstáculo: $-8.0$.
    \end{itemize}
    
    \item \textbf{Navegación y Exploración (si no hay evento terminal):}
    \begin{itemize}
        \item \textbf{Avance en zona libre:} Si la distancia supera \texttt{FRONT\_CLEAR}:
        \begin{itemize}
            \item Acción \texttt{forward}: $+0.3$.
            \item Acción \texttt{slow}: $+0.15$.
        \end{itemize}
        \item \textbf{Penalización por riesgo:} Si la distancia es menor a \texttt{FRONT\_DANGER} y el robot intenta avanzar (\texttt{forward} o \texttt{slow}): $-0.6$.
        \item \textbf{Evasión inteligente:} Si el espacio frontal está bloqueado (menor a \texttt{FRONT\_CLEAR}) y el robot gira hacia el lado con mayor espacio libre: $+0.4$. Esto incentiva evitar "callejones sin salida" antes de entrar en ellos.
    \end{itemize}
\end{enumerate}

\subsubsection{Estados}

\subsubsection{Acciones}
\begin{itemize}
    \item Moverse hacia delante rápido
    \item Moverse hacia delante lento
    \item Moverse fuerte hacia la derecha
    \item Moverse suave hacia la derecha
    \item Moverse fuerte hacia la izquierda
    \item Moverse suave hacia la izquierda
    \item Moverle lento hacia antrás
\end{itemize}


\section{Entorno de simulación y herramientas utilizadas}
Descripción de IsaacSim/IsaacLab: versiones empleadas, configuración inicial y librerías auxiliares. Recursos computacionales (hardware, GPU, sistema operativo).

Configuración de escenarios de entrenamiento (robot, entorno, sensores virtuales, condiciones de interacción).

\subsection{Unity + Visual studio}
El desarrollo del entorno virtual se fundamenta en la integración de Unity \cite{unity3d} y Visual Studio Code \cite{}. Mientras que Unity gestiona la representación visual y las colisiones físicas del entorno, VS Code se utiliza para implementar los scripts de comportamiento de los agentes. Para integrar esta simulación en el ecosistema robótico, se emplea el paquete ROS-TCP-Connector, el cual permite que Unity funcione como un nodo de simulación capaz de publicar datos de sensores y suscribirse a comandos de velocidad desde ROS en tiempo real, superando la barrera de compatibilidad entre el entorno .NET de Unity y el ecosistema Linux de ROS.

\subsection{Visual studio code}
Visual studio code es una IDE de programación de la que hemos hecho uso para realizar el código del trabajo.

\subsection{TensorFlow}
TensorFlow Dashboard \cite{abadi2016tensorflow} es una herramienta que a través de los logs que genera nuestro entrenamiento, va graficando 
cada 4024 pasos, que son 256 por robot, se grafican los resultados.



\subsection{KERAS}
Keras es una biblioteca de código abierto escrita por \cite{chollet2015keras} en Python diseñada para la implementación rápida y eficiente de redes neuronales profundas (Deep Learning). Funciona como una interfaz de alto nivel (API) que se ejecuta sobre frameworks de cálculo tensorial más complejos, principalmente TensorFlow. En este proyecto, Keras ha sido fundamental para simplificar el diseño, entrenamiento y validación de los modelos de aprendizaje utilizados en el JetBot , permitiendo una experimentación ágil gracias a su arquitectura modular.


\subsection{ROS 1}
Hemos empleado ROS1 \cite{quigley2009ros} como pasarela de comunicación entre los robots y el máster, que sería nuestro propio ordenador. Lo que hacemos es 
que cada robot tiene unos tópics propios, sobre los cuales envían y reciben la información.




\subsection{JetBot}
Aunque en nuestros resultados podemos observar que el robot es un cubo amarillo, lo que se esta haciendo es
simular el comportamiento del JetBot \cite{nvidia_jetbot} en un cubo con sus ecuaciones dinámicas


\subsection{Entorno}
Nuestro entorno es un laberinto con paredes y pasillos por donde el robot deberá moverse y encontrar el
objetivo de color rojo que puede estar en cualquier lado de la escena.
Se diseñó un mismo entorno, donde en cada instancia el objetivo estaba en una posición diferente como vemos en 
las siguientes imágenes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./figuras_Nico/img_1_.jpg}
    \caption{Escenario 1}
    \label{fig:img1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./figuras_Nico/img_2.jpg}
    \caption{Escenario 2}
    \label{fig:img2}
\end{figure}







\subsection{Sensores virtuales}
\subsubsection{LIDAR}
Hemos modelado un LIDAR que publica los valores en un topic de ros, es este colisionado
es un código de Unity el cual genera una nube de puntos, y los publica todos a través del tópic. En nuestro código lo tratamos
de manera que lo separamos en 180 sectores, esto lo hacemos cogiendo el mínimo cada dos sectores.



\section{Procedimiento de experimentación / entrenamiento}
Explicación del flujo de trabajo seguido: preparación de modelos, definición de hiperparámetros, duración de entrenamientos, validación de políticas aprendidas.
\subsection{Flujo de trabajo}
Se probó primero definiendo un escenario muy sencillo, donde si el robot giraba en el primer cruce ya alcanzaba
el objetivo final. Esto se modificó y se hicieron 4 escenarios para el mismo entrenamiento, donde el objetivo estaba 
en diferentes puntos.

\subsection{Estrategia de comparacion}
Se definió que cada 1000 pasos en cada robot, se realize una comparación de modelo, pero se detendría
el entrenamineto si la media de los ultimos 1000 episodios no mejoraba durante los 5 siguientes episodios.

\newpage
\section{Programación}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{figuras_Nico/Diagrama_ctrl.png}
    \caption{Diagrama de Flujo del control por LIDAR y cámara}
    \label{fig:my_label}
\end{figure}

%Incluye todas las secciones y subsecciones que tu proyecto necesite para poder entender el trabajo que has realizado