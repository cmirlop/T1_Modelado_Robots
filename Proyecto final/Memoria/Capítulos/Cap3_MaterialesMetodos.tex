% ----------------------------------------------------------------------------------
% ----------------------------------------------------------------------------------
% ----------------------------------------------------------------------------------
% ----------------------------------------------------------------------------------
\chapter{Materiales y métodos}\label{cap:Sol}

\begin{Resumen}

    En este capítulo se describe la metodología integral adoptada para el desarrollo del sistema de navegación autónoma. En primer lugar, se presenta el **modelado dinámico** del robot diferencial utilizando el formalismo de Euler-Lagrange para obtener las ecuaciones de movimiento. Seguidamente, se detalla la estrategia de control basada en **Aprendizaje por Refuerzo Profundo (Deep Reinforcement Learning)**, definiendo la arquitectura del agente, el diseño de la función de recompensa y el procesamiento de la información sensorial (LIDAR y visión). A continuación, se describen las herramientas de **simulación e integración** empleadas (Unity, ROS, Keras/TensorFlow) para crear un entorno de entrenamiento realista. Finalmente, se expone el **procedimiento experimental**, incluyendo el diseño de escenarios y los criterios de validación de las políticas aprendidas.
    
\end{Resumen}

\section{Modelado Dinámico}

El objetivo de esta sección es obtener las ecuaciones que gobiernan el movimiento del robot JetBot 3 relacionando los pares aplicados por los motores con las aceleraciones del sistema. Para ello, se utiliza el formalismo de \textbf{Euler-Lagrange}.

\subsection{Hipótesis Adoptadas}
Para la derivación del modelo matemático, se asumen las siguientes simplificaciones sobre la física del robot, basadas en el modelo estándar de robot diferencial descrito en \cite{siegwart2011intro}:
\begin{itemize}
    \item \textbf{Cuerpo Rígido:} El chasis y las ruedas son indeformables.
    \item \textbf{Movimiento Plano:} El robot se desplaza sobre una superficie horizontal, por lo que la energía potencial gravitatoria es constante ($V=0$).
    \item \textbf{Rodadura Pura:} Se asume la condición de no deslizamiento longitudinal ni lateral en las ruedas (restricción no holonómica).
    \item \textbf{Simetría:} Se asume que el centro de masa (CoM) del robot se encuentra en el punto medio del eje que une las dos ruedas motrices.
\end{itemize}

\subsection{Definición de Coordenadas}
La postura del robot en el entorno global se define mediante el vector de coordenadas generalizadas $q$:

\begin{equation}
    q = \begin{bmatrix} x \\ y \\ \theta \end{bmatrix}
\end{equation}

Donde $(x, y)$ representan la posición cartesiana del punto medio del eje de las ruedas y $\theta$ la orientación del chasis respecto al eje $X$ global.

\begin{figure}[h]
    \centering
    % Aquí insertas tu diagrama generado con TikZ o la imagen
    \caption{Esquema cinemático del robot diferencial (JetBot) con las variables de estado y parámetros geométricos.}
    \label{fig:robot_scheme}
\end{figure}

\subsection{Formulación de Euler-Lagrange}
La función Lagrangiana $\mathcal{L}$ se define como la diferencia entre la energía cinética ($T$) y la energía potencial ($V$) del sistema:

\begin{equation}
    \mathcal{L}(q, \dot{q}) = T(q, \dot{q}) - V(q)
\end{equation}

Dado que el movimiento es plano ($V=0$), el Lagrangiano es igual a la energía cinética total, compuesta por la traslación lineal y la rotación angular:

\begin{equation}
    \mathcal{L} = \frac{1}{2} m (\dot{x}^2 + \dot{y}^2) + \frac{1}{2} I_z \dot{\theta}^2
\end{equation}

Siendo $m$ la masa total del robot e $I_z$ el momento de inercia respecto al eje vertical que pasa por el centro de masas.

Las ecuaciones de movimiento se obtienen aplicando la ecuación de Euler-Lagrange:

\begin{equation}
    \frac{d}{dt} \left( \frac{\partial \mathcal{L}}{\partial \dot{q}} \right) - \frac{\partial \mathcal{L}}{\partial q} = B(q)\tau - A^T(q)\lambda
\end{equation}

Donde $\tau$ son las fuerzas generalizadas (pares de los motores), $B(q)$ es la matriz de transformación de entrada y $A^T(q)\lambda$ representa las fuerzas de restricción (fricción de rodadura lateral) que limitan el movimiento a la dirección de las ruedas.

\subsection{Modelo Dinámico Resultante}
Resolviendo las derivadas y considerando la relación cinemática entre la velocidad del robot y la velocidad angular de las ruedas ($\dot{\phi}_R, \dot{\phi}_L$), se llega a la forma compacta del modelo dinámico en el espacio de estados:

\begin{equation}
    \begin{bmatrix} 
    m & 0 & 0 \\ 
    0 & m & 0 \\ 
    0 & 0 & I_z 
    \end{bmatrix}
    \begin{bmatrix} \ddot{x} \\ \ddot{y} \\ \ddot{\theta} \end{bmatrix}
    + \mathbf{F}_{fric}(\dot{q}) 
    = \frac{1}{R}
    \begin{bmatrix} 
    \cos\theta & \cos\theta \\ 
    \sin\theta & \sin\theta \\ 
    L/2 & -L/2 
    \end{bmatrix}
    \begin{bmatrix} \tau_R \\ \tau_L \end{bmatrix}
    \label{eq:final_dynamic}
\end{equation}

Donde:
\begin{itemize}
    \item $R$: Radio de las ruedas.
    \item $L$: Distancia entre las ruedas (ancho de vía).
    \item $\tau_R, \tau_L$: Pares aplicados por los motores derecho e izquierdo.
\end{itemize}

Esta ecuación (\ref{eq:final_dynamic}) describe cómo los pares de los motores aceleran al robot, sujeto a su masa e inercia \cite{siegwart2011intro}.
%Esta ecuación (\ref{eq:final_dynamic}) describe cómo los pares de los motores aceleran al robot, sujeto a su masa e inercia \cite{siegwart2011intro}.

\section{Metodología de control y aprendizaje por refuerzo (RL)}

\subsection{Algoritmo de control por LIDAR y cámara}

El software desarrollado implementa un controlador reactivo para un robot móvil diferencial operado bajo el middleware ROS (Robot Operating System) \cite{quigley2009ros}. El sistema integra capacidades de percepción multimodal (LiDAR y visión artificial) para la navegación autónoma en laberintos y la localización de objetivos visuales, siguiendo un paradigma de comportamiento basado en prioridades \cite{arkin1998behavior}.

\subsubsection{Arquitectura del Sistema}
El script utiliza la librería \texttt{multiprocessing} para instanciar múltiples procesos de control de forma paralela. Esto permite simular o controlar varios agentes simultáneamente, donde cada proceso ejecuta una instancia de la clase \texttt{MazeLidarRedController}. Esta arquitectura modular facilita la escalabilidad y el aislamiento de fallos entre agentes.

\subsection{Procesamiento de Percepción}

\subsubsection{Visión Artificial (Detección de Objetivo)}
El procesamiento de imágenes se realiza en la función \texttt{extrae\_objetivo\_rojo}, utilizando la librería OpenCV \cite{bradski2008learning}. El flujo de trabajo es el siguiente:

\begin{enumerate}
    \item \textbf{Preprocesamiento y Cambio de Espacio de Color:} La imagen recibida se redimensiona a 320 px y se convierte del espacio BGR al espacio HSV (Hue, Saturation, Value). El uso de HSV es preferible en robótica móvil ya que separa la información cromática (Matiz) de la intensidad luminosa (Valor), haciendo la detección más robusta a sombras y cambios de luz \cite{siegwart2011introduction}.
    
    \item \textbf{Segmentación de Color:} Se aplican máscaras de umbralización para aislar el color rojo. Dado que el matiz rojo en HSV envuelve el espectro ($0^\circ$ y $180^\circ$), se combinan dos rangos:
    \begin{itemize}
        \item Rango bajo: $H \in [0, 10]$
        \item Rango alto: $H \in [170, 180]$
    \end{itemize}
    
    \item \textbf{Filtrado Morfológico:} Se aplican operaciones de apertura y cierre (\textit{morphological opening/closing}) para eliminar el ruido impulsivo y conectar componentes disjuntos del objeto detectado \cite{gonzalez2008digital}.
    
    \item \textbf{Cálculo de Error:} Se extrae el contorno de mayor área. Si supera el umbral $\texttt{AREA\_MIN}$, se calcula el centroide $(c_x, c_y)$ y el error de orientación normalizado (\textit{bearing}):
    \begin{equation}
        \text{bearing} = \frac{c_x - \frac{W}{2}}{\frac{W}{2}} \quad \in [-1, 1]
    \end{equation}
    Este valor es fundamental para el lazo de control visual o \textit{Visual Servoing} \cite{chaumette2006visual}.
\end{enumerate}

\subsubsection{Lectura del LIDAR}
El sensor láser se utiliza para obtener distancias de seguridad mediante la función \texttt{min\_range\_at\_angle}. Esta función procesa la nube de puntos del \texttt{LaserScan} y aplica un filtro de media en ventanas angulares específicas para reducir el ruido de medida del sensor:
\begin{itemize}
    \item \textbf{Frontal ($0$ rad):} Detección de colisiones inminentes.
    \item \textbf{Laterales ($\pm\pi/2$ rad):} Navegación y seguimiento de paredes.
\end{itemize}

\subsection{Lógica de Control Reactivo}
El controlador opera en un bucle de 10 Hz, implementando una arquitectura de subsunción simplificada donde las capas inferiores (seguridad) pueden inhibir a las superiores (navegación) \cite{brooks1986robust}:

\subsubsection{Prioridad 1: Evasión de Colisiones (Seguridad)}
Independientemente del objetivo, si la distancia frontal es inferior a $\texttt{dist\_wall\_close}$, el robot entra en estado de evasión. Se detiene el avance lineal ($v=0$) y se aplica una velocidad angular $\omega$ hacia la dirección con mayor espacio libre (izquierda o derecha).

\subsubsection{Prioridad 2: Navegación Visual (Visual Servoing)}
Si el entorno es seguro y se detecta el objetivo, el robot cambia a un control basado en visión:
\begin{itemize}
    \item \textbf{Control Angular:} Se aplica un controlador proporcional (P) para centrar el objetivo en la imagen:
    \begin{equation}
        \omega_{cmd} = K_{ang} \cdot \text{bearing}
    \end{equation}
    Donde $K_{ang}$ es la ganancia proporcional. Se incluye saturación y zona muerta para proteger los actuadores.
    \item \textbf{Control Lineal:} La velocidad se reduce a medida que el área del objetivo (proxy de la distancia) aumenta. Si el área supera $\texttt{AREA\_GOAL}$ y $|\text{bearing}| < \texttt{CENTER\_THRESH}$, el robot se detiene, considerando la meta alcanzada.
\end{itemize}

\subsubsection{Prioridad 3: Exploración (Seguimiento de Pared)}
En ausencia de estímulos visuales o peligros inmediatos, el robot ejecuta un algoritmo de seguimiento de pared derecha (\textit{Wall Following}). Este comportamiento permite recorrer el laberinto manteniendo una distancia constante a la pared, garantizando la cobertura del espacio sin necesidad de un mapa global \cite{siegwart2011introduction}.

\subsection{Algoritmo RL: PPO (Proximal Policy Optimization)}

\subsection*{Definición del Entorno y Agente}

El sistema implementado utiliza un enfoque de aprendizaje por refuerzo profundo (\textit{Deep Reinforcement Learning}). El agente, basado en el algoritmo PPO (Proximal Policy Optimization) \cite{schulman2017proximal}, interactúa con el entorno a través de un Modelo de Decisión de Markov (MDP). A diferencia de enfoques como DQN, PPO opera bajo una arquitectura Actor-Crítico, optimizando la política de control de manera iterativa y estable.

A continuación se detallan los tres componentes principales del MDP diseñado:

\subsubsection{Espacio de Acciones}
El control del robot se ha discretizado para facilitar el aprendizaje de la red neuronal. El espacio de acciones $\mathcal{A}$ consta de 7 comandos posibles, definidos por pares de velocidad lineal ($v$) y angular ($w$):

\begin{itemize}
    \item \textbf{Avance:}
    \begin{itemize}
        \item \texttt{forward}: Avance rápido ($v=3.0$ m/s, $w=0.0$ rad/s).
        \item \texttt{slow}: Avance lento ($v=1.0$ m/s, $w=0.0$ rad/s).
    \end{itemize}
    \item \textbf{Giros Estáticos:}
    \begin{itemize}
        \item \texttt{left}: Giro a la izquierda ($v=0.0$, $w=-1.0$).
        \item \texttt{right}: Giro a la derecha ($v=0.0$, $w=1.0$).
    \end{itemize}
    \item \textbf{Giros Suaves (Curvas):}
    \begin{itemize}
        \item \texttt{left\_soft}: Giro suave izquierda ($v=0.0$, $w=-0.5$).
        \item \texttt{right\_soft}: Giro suave derecha ($v=0.0$, $w=0.5$).
    \end{itemize}
    \item \textbf{Retroceso:}
    \begin{itemize}
        \item \texttt{back}: Maniobra de recuperación ($v=-0.1$, $w=0.0$).
    \end{itemize}
\end{itemize}

\subsubsection{Espacio de Observación (Estados)}
El vector de estado $s_t$ proporciona al agente una visión multimodal del entorno. Las observaciones se normalizan y concatenan para formar un vector de entrada de dimensión aproximada 188, compuesto por:

\begin{enumerate}
    \item \textbf{LiDAR ($L$):} Lecturas de distancia de 180 sectores. Los valores se normalizan al rango $[0, 1]$ dividiéndolos por la distancia máxima de recorte (\texttt{CLIP\_DIST} $= 30$ m).
    \item \textbf{Visión ($V$):} Información extraída del procesamiento de imagen para el objetivo rojo:
    \begin{itemize}
        \item \textit{Bearing} (Orientación): Valor continuo en $[-1, 1]$ indicando la desviación horizontal del objetivo respecto al centro.
        \item \textit{Área}: Tamaño relativo del objeto detectado, normalizado en $[0, 1]$.
    \end{itemize}
    \item \textbf{Flags de Bloqueo ($F$):} Indicadores binarios que alertan de obstáculos cercanos (distancia $< 5$ m) en las zonas frontal y laterales.
    \item \textbf{Odometría ($P$):} Coordenadas globales $(x, y, z)$ del robot.
\end{enumerate}

\subsubsection{Función de Recompensa}
La función de recompensa $R(s, a, s')$ es densa e incluye lógica basada en umbrales de seguridad definidos por el sensor LiDAR. 

\paragraph{Zonas de Seguridad (LiDAR)}
Se han definido tres zonas críticas basadas en la distancia frontal detectada:
\begin{itemize}
    \item \texttt{FRONT\_CLEAR} ($> 10$ m): Espacio frontal libre para navegar a alta velocidad.
    \item \texttt{FRONT\_DANGER} ($< 7$ m): Zona de precaución donde el avance es peligroso.
    \item \texttt{CRITICAL} ($< 5.5$ m): Distancia inminente de choque; se prioriza el retroceso.
\end{itemize}

\paragraph{Cálculo de Recompensas}
La recompensa total se asigna según la siguiente lógica priorizada:

\begin{enumerate}
    \item \textbf{Eventos Terminales:}
    \begin{itemize}
        \item \textbf{Éxito:} Si se alcanza el objetivo (área visual mínima detectada): $+30.0$.
        \item \textbf{Colisión:} Si el robot choca con un obstáculo: $-8.0$.
    \end{itemize}
    
    \item \textbf{Navegación y Exploración (si no hay evento terminal):}
    \begin{itemize}
        \item \textbf{Avance en zona libre:} Si la distancia supera \texttt{FRONT\_CLEAR}:
        \begin{itemize}
            \item Acción \texttt{forward}: $+0.3$.
            \item Acción \texttt{slow}: $+0.15$.
        \end{itemize}
        \item \textbf{Penalización por riesgo:} Si la distancia es menor a \texttt{FRONT\_DANGER} y el robot intenta avanzar (\texttt{forward} o \texttt{slow}): $-0.6$.
        \item \textbf{Evasión inteligente:} Si el espacio frontal está bloqueado (menor a \texttt{FRONT\_CLEAR}) y el robot gira hacia el lado con mayor espacio libre: $+0.4$. Esto incentiva evitar "callejones sin salida" antes de entrar en ellos.
    \end{itemize}
\end{enumerate}

\subsubsection{Acciones}
\begin{itemize}
    \item Moverse hacia delante rápido
    \item Moverse hacia delante lento
    \item Moverse fuerte hacia la derecha
    \item Moverse suave hacia la derecha
    \item Moverse fuerte hacia la izquierda
    \item Moverse suave hacia la izquierda
    \item Moverle lento hacia antrás
\end{itemize}


\section{Entorno de simulación y herramientas utilizadas}
\subsection{Unity + Visual studio}
El desarrollo del entorno virtual se fundamenta en la integración de \cite{unity3d} y Visual Studio Code \cite{vscode}. Mientras que Unity gestiona la representación visual y las colisiones físicas del entorno, VS Code se utiliza para implementar los scripts de comportamiento de los agentes. Para integrar esta simulación en el ecosistema robótico, se emplea el paquete ROS-TCP-Connector, el cual permite que Unity funcione como un nodo de simulación capaz de publicar datos de sensores y suscribirse a comandos de velocidad desde ROS en tiempo real, superando la barrera de compatibilidad entre el entorno .NET de Unity y el ecosistema Linux de ROS.

\subsection{Visual studio code}
Visual studio code es una IDE de programación de la que hemos hecho uso para realizar el código del trabajo.

\subsection{TensorFlow}
TensorFlow Dashboard \cite{abadi2016tensorflow} es una herramienta que a través de los logs que genera nuestro entrenamiento, va graficando 
cada 4024 pasos, que son 256 por robot, se grafican los resultados.



\subsection{KERAS}
Keras es una biblioteca de código abierto escrita por \cite{chollet2015keras} en Python diseñada para la implementación rápida y eficiente de redes neuronales profundas (Deep Learning). Funciona como una interfaz de alto nivel (API) que se ejecuta sobre frameworks de cálculo tensorial más complejos, principalmente TensorFlow. En este proyecto, Keras ha sido fundamental para simplificar el diseño, entrenamiento y validación de los modelos de aprendizaje utilizados en el JetBot , permitiendo una experimentación ágil gracias a su arquitectura modular.


\subsection{ROS 1}
Hemos empleado ROS1 \cite{quigley2009ros} como pasarela de comunicación entre los robots y el máster, que sería nuestro propio ordenador. Lo que hacemos es 
que cada robot tiene unos tópics propios, sobre los cuales envían y reciben la información.




\subsection{JetBot}
Aunque en nuestros resultados podemos observar que el robot es un cubo amarillo, lo que se esta haciendo es
simular el comportamiento del JetBot \cite{nvidia_jetbot} en un cubo con sus ecuaciones dinámicas


\subsection{Entorno}
Nuestro entorno es un laberinto con paredes y pasillos por donde el robot deberá moverse y encontrar el
objetivo de color rojo que puede estar en cualquier lado de la escena.
Se diseñó un mismo entorno, donde en cada instancia el objetivo estaba en una posición diferente como vemos en 
las siguientes imágenes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./figuras_Nico/img_1_.jpg}
    \caption{Escenario 1}
    \label{fig:img1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./figuras_Nico/img_2.jpg}
    \caption{Escenario 2}
    \label{fig:img2}
\end{figure}







\subsection{Sensores virtuales}
\subsubsection{LIDAR}
Hemos modelado un LIDAR que publica los valores en un topic de ros, es este colisionado
es un código de Unity el cual genera una nube de puntos, y los publica todos a través del tópic. En nuestro código lo tratamos
de manera que lo separamos en 180 sectores, esto lo hacemos cogiendo el mínimo cada dos sectores.



\section{Procedimiento de experimentación / entrenamiento}
\subsection{Flujo de trabajo}
Se probó primero definiendo un escenario muy sencillo, donde si el robot giraba en el primer cruce ya alcanzaba
el objetivo final. Esto se modificó y se hicieron 4 escenarios para el mismo entrenamiento, donde el objetivo estaba 
en diferentes puntos.

\subsection{Estrategia de comparacion}
Se definió que cada 1000 pasos en cada robot, se realize una comparación de modelo, pero se detendría
el entrenamineto si la media de los ultimos 1000 episodios no mejoraba durante los 5 siguientes episodios.

\newpage
\section{Programación}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{figuras_Nico/Diagrama_ctrl.png}
    \caption{Diagrama de Flujo del control por LIDAR y cámara}
    \label{fig:my_label}
\end{figure}

%Incluye todas las secciones y subsecciones que tu proyecto necesite para poder entender el trabajo que has realizado